















# 理论学习

## P1.什么是强化学习

通过价值选行为

Q learning

Saras

Deep Q Network

直接选行为

Policy Gradients

想象环境并从中学习

Model based RL

## P2.强化学习方法汇总

不理解环境（Model-Free RL）、理解环境（Model-Based RL）

基于概率（Policy-Based RL）、基于价值（Value-Based RL）

回合更新（Monte-Carlo update）、单步更新（Temporal-Difference update）

在线学习（On-Policy）、离线学习（Off-Policy）



## P4.要求准备

Tkinter、OpenAI gym:模拟环境

## P5Q learning



参数定义

![1586705106579](强化学习.assets\1586705106579.png)



epsilon：90%概率选择最大值，其余则随机选择

a ：alpha：学习率

r：lambda：未来值的期望



- 初始化

def build_q_table()

- 选择动作

def choose_action()

- 环境反应

def get_env_feedback()



Q(s,a)=Q(s,a)+a[R(s_next)+r*Max(Q(s_next)-Q(s,a)]

现实值：R(s_next)+r*Max(Q(s_next)

预估值：Q(s,a)

目前值+ 学习率*（现实值-目前值）



# 实践

## GYM

```python
env=gym.make("MountainCar-v0") #创建对应的游戏环境
env.seed(1) #可选，设置随机数，以便让过程重现
env=env.unwrapped #可选，为环境增加限制，对训练有利

#环境操作
env.reset()#重置环境
env.render() #展示环境
s_,r,done,info=env.step(a) #环境返回执行动作a后的下一个状态、奖励值、是否终止以及其他信息

#环境空间
env.observation_space.shape

#动作
env.action_space.n#动作个数
env.action_space.sample()#随机动作

```



## DQN

### 架构

#### 记忆库

```python
#定义类型
Experience = collections.namedtuple(  
	'Experience',
    field_names=['state', 'action', 'reward',  'done', 'new_state'])  

class Memory:
    #初始化，设置最大存储
    def __init__(self,capacity:int)
    #增加数据
    def append(self,exp:Experience)
    #随机读取数据
    def sample(self,batch_size:int)
```

#### 神经网络

```python
class Net(nn.Module):
    #初始化
    def __init__(self):      
    #向前传播
    def forward(self,x):      
```

#### DQN模型

```python
class DQN(object):
    #初始化
    def __init__(self,n_actions,BATCH_SIZE,MEMORY_CAPYCITY,TARGET_REPLACE_ITER):
        eval_net，target_net=Net()，Net()#定义eval及target网络
        eval_net.train()，target_net.eval()#eval学习，target不学习
        
        memory = Memory(MEMORY_CAPYCITY)#初始化记忆库
        
        GAMMA=0.9#学习率
        target_replace_iter#替换target数字的频次
        
        
        loss_func=nn.SmoothL1Loss()#损失值函数
        
    #随机选择动作：相关参数：epsilon:0.9
    def choose_action(self,x):
        if np.random.uniform()<epsilon:
            #进入神经网络，选择最大action
            action_value=self.eval_net.forward(x)
        else：
        	#随机选择action
            
    #保存经验
    def store_experience(self,exp:Experience):
    #学习
    def learn(self):
        #达到target频次，更新target_net网络权值
        if learn_step_counter % TARGET_REPLACE_ITER == 0:
            target_net.load_state_dict(eval_net.state_dict())
        #读取记忆库
        states, actions, rewards, dones, new_states=self.memory.sample(self.BATCH_SIZE)
        
        #预测：获取action对应的eval_net数值
        q_eval = self.eval_net(states).gather(1,actions.unsqueeze(-1)).squeeze(-1)
        #现实：R+学习率*q_next
        q_next = self.target_net(new_states).max(1)[0]
        q_target = rewards +  GAMMA*q_next
        #损失值
        loss = self.loss_func(q_eval,q_target)
        
        #向后传播
        loss.backward()
        #限制eval_net网络的范围
        for param in self.eval_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
```



#### 训练

```python
#优化环境画面
def preprocess_img(img):
    return np.mean(img,axis=2)[100:200,10:150][::2,::2]

#游戏训练过程
for i in range(epochs):
   		#初始化
        s=env.reset()
        #添加4次环境画面
        input_buf=[]
        for _ in range(4):input_buf.append(preprocess_img(s))
      
        while True:
            #选择一个动作
            a=dqn.choose_action(input_buf)
            for _ in range(4):
                #获取环境反馈
                s_,r,done,info=env.step(a)
                next_input_buf.append(preprocess_img(s_))
                #优化r，提升学习效率
                #待补充~~~~
            
            #存储记忆
            exp=Experience(input_buf,a,total_r,done,next_input_buf)
            dqn.store_experience(exp)
            #大于门限开始学习
            if dqn.memory_counter>MEMORY_CAPYCITY:
                dqn.learn()
            #游戏结束
            if done:
                break
            #进入下次学习
            input_buf=next_input_buf
```



### 代码（待校正）

#### DQN.py

```python
import torch
import torch.nn as nn
import torch.optim as optim
from  torch.autograd import Variable
import os

import numpy as np

import collections


Experience = collections.namedtuple(  
 'Experience', field_names=['state', 'action', 'reward',  
 'done', 'new_state'])  

class Memory:
    def __init__(self,capacity:int):
        self.buffer=collections.deque(maxlen=capacity)
    def append(self,exp:Experience):
        self.buffer.append(exp)
    def sample(self,batch_size:int):
        indices = np.random.choice(len(self.buffer), batch_size, replace=False) 
        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices]) 
        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),  
                np.array(dones, dtype=np.int), np.array(next_states))


class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.conv1=nn.Sequential(
            nn.Conv2d(4,16,(4,4),(5,7)),
            nn.BatchNorm2d(16),
            nn.ReLU()
        )
        
        self.conv2=nn.Sequential(
            nn.Conv2d(16,32,2,1),
            nn.BatchNorm2d(32),
            nn.ReLU()
        )
        
        self.fc1=nn.Sequential(
            nn.Flatten(),
            nn.Linear(2592,128),
            nn.ReLU()
            
        )
        self.fc2=nn.Sequential(
            nn.Linear(128,4),
            nn.Softmax()
        )
        #self.apply(self.init_weights)
        
    def forward(self,x):
        x=self.conv1(x)
        x=self.conv2(x)
        x=self.fc1(x)
        x=self.fc2(x)
        return x
    
    def init_weights(self, m):
        if type(m) == nn.Conv2d:
            m.weight.data.normal_(0.0, 0.02)
        if type(m) == nn.Linear:
            torch.nn.init.xavier_uniform_(m.weight)
            m.bias.data.fill_(0.01)
            
class DQN(object):
    def __init__(self,n_actions,BATCH_SIZE,MEMORY_CAPYCITY,TARGET_REPLACE_ITER):
        #
        
        self.eval_net=Net()
        self.target_net=Net()
        
        
        
        self.eval_net.train()
        self.target_net.eval()
        
        if os.path.exists("weight_eval_net.pth"):
            print("加载weight")
            self.eval_net.load_state_dict(torch.load("weight_eval_net.pth"))
            self.target_net.load_state_dict(torch.load("weight_eval_net.pth"))
        
        
        self.N_ACTIONS=n_actions
        self.EPSILON=1
        self.DIFF1=(1.0 - 0.1)/10e5
        #self.DIFF2=(0.1 - 0.01)/10e5
        
        
        self.BATCH_SIZE=BATCH_SIZE
        
        self.memory_counter=0
        self.memory = Memory(MEMORY_CAPYCITY)
        
        self.learn_step_counter=0
        self.TARGET_REPLACE_ITER =TARGET_REPLACE_ITER
        
        #NN
        self.optimizer=optim.Adam(self.eval_net.parameters(),lr=0.0001)#lr=0.0000625
        #self.loss_func=nn.MSELoss()
        #self.loss_func=nn.MSELoss()
        self.loss_func=nn.SmoothL1Loss()
    
    def choose_action(self,x):
        #选择动作
        #转换维度位置成(3,w,h)
        #x=x.transpose(2,0,1)
        
        x = torch.unsqueeze(torch.FloatTensor(x), 0)#在第一个位置,添加一个唯独
        x=Variable(x)
        if np.random.uniform()<self.EPSILON:
            action_value=np.random.randint(0,self.N_ACTIONS)
        else:
            action_value=self.eval_net.forward(x)
            action_value=torch.argmax(action_value).item()
        self.EPSILON-=self.DIFF1
        #print(self.EPSILON)
        return action_value
    
    def store_experience(self,exp:Experience):
        #存储记忆
        self.memory.append(exp)
        self.memory_counter+=1
    
    def learn(self):
        # target 网络更新
        # 学习记忆库中的记忆
        if self.learn_step_counter % self.TARGET_REPLACE_ITER == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
            #print("保存weight")
            torch.save(self.eval_net.state_dict(),"weight_eval_net.pth")
            torch.save(self.target_net.state_dict(),"weight_target_net.pth")
        
        self.learn_step_counter += 1
        
        #待确认
        #self.eval_net.zero_grad()
        self.optimizer.zero_grad()
        
        states, actions, rewards, dones, new_states=self.memory.sample(self.BATCH_SIZE)
        
        #转换成(batch_size,维度,w,h)
        #states=states.transpose(0,3,1,2)
        #new_states=new_states.transpose(0,3,1,2)
        
        states=Variable(torch.FloatTensor(states))
        new_states=Variable(torch.FloatTensor(new_states))
        actions=Variable(torch.LongTensor(actions))
        rewards=Variable(torch.FloatTensor(rewards))
        #done_mask = torch.ByteTensor(dones)
        
        q_eval = self.eval_net(states).gather(1,actions.unsqueeze(-1)).squeeze(-1)
        q_next = self.target_net(new_states).max(1)[0]
        #q_next[done_mask]=0.0
        q_target = rewards + 0.9 *q_next
        
        loss = self.loss_func(q_eval,q_target)
        
        if self.learn_step_counter % self.TARGET_REPLACE_ITER == 0:
            print("loss:",loss.item(),"epsilon:",self.EPSILON)

        
        loss.backward()
        for param in self.eval_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
```

#### Train.py

```python
import argparse
import numpy as np
import gym
#from tqdm.notebook import tqdm
from tqdm import tqdm
from DQN import Net,DQN,Experience

def arg_parse():
    parser = argparse.ArgumentParser(description='YOLO v3 Detection Module')

    parser.add_argument("--epochs",
                        dest = 'epochs',
                        help = "训练次数",
                        default = 10,
                        type = int
                       )
    
    return parser.parse_args()

def preprocess_img(img):
    return np.mean(img,axis=2)[100:200,10:150][::2,::2]

def train(dqn,epochs):
    qbar=tqdm(total=epochs)
    print("epochs:",epochs) 
    for i in range(epochs):
        s=env.reset()
        #s=preprocess_img(s)#shape:(50,70)
        input_buf=[]
        for _ in range(4):input_buf.append(preprocess_img(s))
         
        qbar.update(1)
        game_score=0
        while True:
            
            total_r=0
            next_input_buf=[]
            
            
            #选择一个动作
            a=dqn.choose_action(input_buf)
            for _ in range(4):
                #获取环境反馈
                s_,r,done,info=env.step(a)
                next_input_buf.append(preprocess_img(s_))
                
                if r==1:
                    game_score+=1
                # 优化r
                screen_point=s_[0:89,:,:]
                point_0=np.unravel_index(np.argmax(screen_point),screen_point.shape)
                if(point_0[1]+point_0[0]==0):
                    r=0
                else:
                    screen_heng=s_[89:100,:,:]
                    point_1=np.unravel_index(np.argmax(screen_heng),screen_heng.shape)
                    r=(200-np.abs(point_0[1]-point_1[1])-2)/200
                total_r+=r
            
            #存储记忆
            exp=Experience(input_buf,a,total_r,done,next_input_buf)
            dqn.store_experience(exp)
            
            if dqn.memory_counter>MEMORY_CAPYCITY:
                dqn.learn()
            if done:
                print("结束,得分:",game_score)
                break
            input_buf=next_input_buf


            
if __name__=="__main__":
    #获取main参数
    args = arg_parse()
    epochs=args.epochs
    
    #初始化环境
    env = gym.make('Breakout-v0')
    n_actions=env.action_space.n
    
    #设置参数
    MEMORY_CAPYCITY=2000
    TARGET_REPLACE_ITER=100#Q 现实网络的更新频率
    BATCH_SIZE=32

    dqn=DQN(n_actions,BATCH_SIZE,MEMORY_CAPYCITY,TARGET_REPLACE_ITER)
    
    train(dqn,epochs)
```

#### 运行

```python
from DQN import Net,DQN,Experience
%run train.py --epochs=30
```









