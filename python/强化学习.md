# GYM

```python
env=gym.make("MountainCar-v0") #创建对应的游戏环境
env.seed(1) #可选，设置随机数，以便让过程重现
env=env.unwrapped #可选，为环境增加限制，对训练有利

#环境操作
env.reset()#重置环境
env.render() #展示环境
s_,r,done,info=env.step(a) #环境返回执行动作a后的下一个状态、奖励值、是否终止以及其他信息

#环境空间
env.observation_space.shape

#动作
env.action_space.n#动作个数
env.action_space.sample()#随机动作

```



# P1.什么是强化学习

通过价值选行为

Q learning

Saras

Deep Q Network

直接选行为

Policy Gradients

想象环境并从中学习

Model based RL

# P2.强化学习方法汇总

不理解环境（Model-Free RL）、理解环境（Model-Based RL）

基于概率（Policy-Based RL）、基于价值（Value-Based RL）

回合更新（Monte-Carlo update）、单步更新（Temporal-Difference update）

在线学习（On-Policy）、离线学习（Off-Policy）



# P4.要求准备

Tkinter、OpenAI gym:模拟环境

# P5Q learning



参数定义

![1586705106579](强化学习.assets\1586705106579.png)



epsilon：90%概率选择最大值，其余则随机选择

a ：alpha：学习率

r：lambda：未来值的期望



- 初始化

def build_q_table()

- 选择动作

def choose_action()

- 环境反应

def get_env_feedback()



Q(s,a)=Q(s,a)+a[R(s_next)+r*Max(Q(s_next)-Q(s,a)]

现实值：R(s_next)+r*Max(Q(s_next)

预估值：Q(s,a)

目前值+ 学习率*（现实值-目前值）

## 算法更新

## 思维决策



